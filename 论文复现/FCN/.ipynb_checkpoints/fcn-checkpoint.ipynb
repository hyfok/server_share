{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms, models\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=512 #大概需要2G的显存\n",
    "EPOCHS=20 # 总共训练批次\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, stride=stride, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyFCN_diy_all(nn.Module):\n",
    "    def __init__(self, ResidualBlock=ResidualBlock, num_classes=30):\n",
    "        super().__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  1, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 1, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 1, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 1, stride=2)\n",
    "        #self.fc = nn.Linear(512, 10)\n",
    "        \n",
    "        \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        #out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.resnet50(pretrained=True)\n",
    "ch = list(m.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ch)):\n",
    "    print(ch[i]) #0 3 5 6 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    '''\n",
    "    return a bilinear filter tensor\n",
    "    '''\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype='float32')\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).cuda()\n",
    "\n",
    "class MyFCN(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        self.n_class = n_class\n",
    "        super().__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        self.layer00 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=100, bias=False)\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[1:4]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(*self.base_layers[4]) # size=(N, 256, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(256, 256, 1, 0)\n",
    "        \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 512, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(512, 512, 1, 0)\n",
    "        \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 1024, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(1024, 1024, 1, 0)\n",
    "        \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 2048, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(2048, 2048, 1, 0)\n",
    "        \n",
    "        self.conv0 = convrelu(2048, 4096, 7, 0)\n",
    "        #self.conv0 = convrelu(2048, 4096, 1, 0)\n",
    "        self.conv1 = convrelu(4096, 4096, 1, 0)\n",
    "        \n",
    "        self.scores1 = nn.Conv2d(4096, n_class, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.scores2 = nn.Conv2d(1024, n_class, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.scores3 = nn.Conv2d(512, n_class, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.upsample_8x = nn.ConvTranspose2d(n_class, n_class, 16, 8, 4, bias=False)\n",
    "        self.upsample_8x.weight.data = bilinear_kernel(n_class, n_class, 16) # 使用双线性 kernel\n",
    "        \n",
    "        self.upsample_4x = nn.ConvTranspose2d(n_class, n_class, 4, 2, 1, bias=False)\n",
    "        self.upsample_4x.weight.data = bilinear_kernel(n_class, n_class, 4) # 使用双线性 kernel\n",
    "        \n",
    "        self.upsample_2x = nn.ConvTranspose2d(n_class, n_class, 4, 2, 1, bias=False)   \n",
    "        self.upsample_2x.weight.data = bilinear_kernel(n_class, n_class, 4) # 使用双线性 kernel\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # the base net\n",
    "        layer0 = self.layer00(input)\n",
    "        layer0 = self.layer0(layer0)\n",
    "        layer0 = self.layer0_1x1(layer0) \n",
    "        \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        \n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        \n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "\n",
    "        # fully conv\n",
    "        fc5 = self.conv0(layer4)\n",
    "        fc5 = F.dropout(fc5,p=0.5,inplace=True)\n",
    "        fc5 = self.conv1(fc5)\n",
    "        fc5 = F.dropout(fc5,p=0.5,inplace=True)\n",
    "        fc5 = self.scores1(fc5)\n",
    "        fc5 = self.upsample_2x(fc5)\n",
    "        \n",
    "        fc6 = self.scores2(layer3)\n",
    "        fc6 = fc6[:,:,abs(fc6.size()[2]-fc5.size()[2])//2:fc5.size()[2]+abs(fc6.size()[2]-fc5.size()[2])//2,abs(fc6.size()[3]-fc5.size()[3])//2:fc5.size()[3]+abs(fc6.size()[3]-fc5.size()[3])//2]\n",
    "        fc6 = fc6 + fc5\n",
    "        fc6 = self.upsample_4x(fc6)\n",
    "\n",
    "        fc7 = self.scores3(layer2)\n",
    "        fc7 = fc7[:,:,abs(fc7.size()[2]-fc6.size()[2])//2:fc6.size()[2]+abs(fc7.size()[2]-fc6.size()[2])//2,abs(fc7.size()[3]-fc6.size()[3])//2:fc6.size()[3]+abs(fc7.size()[3]-fc6.size()[3])//2]\n",
    "        fc7 = fc7 + fc6\n",
    "        fc7 = self.upsample_8x(fc7)\n",
    "        \n",
    "        out = fc7[:,:,abs(fc7.size()[2]-input.size()[2])//2:input.size()[2]+abs(fc7.size()[2]-input.size()[2])//2,abs(fc7.size()[3]-input.size()[3])//2:input.size()[3]+abs(fc7.size()[3]-input.size()[3])//2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 209, 209]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 209, 209]             128\n",
      "       BatchNorm2d-3         [-1, 64, 209, 209]             128\n",
      "              ReLU-4         [-1, 64, 209, 209]               0\n",
      "              ReLU-5         [-1, 64, 209, 209]               0\n",
      "         MaxPool2d-6         [-1, 64, 105, 105]               0\n",
      "         MaxPool2d-7         [-1, 64, 105, 105]               0\n",
      "            Conv2d-8         [-1, 64, 105, 105]           4,160\n",
      "              ReLU-9         [-1, 64, 105, 105]               0\n",
      "           Conv2d-10         [-1, 64, 105, 105]           4,096\n",
      "           Conv2d-11         [-1, 64, 105, 105]           4,096\n",
      "      BatchNorm2d-12         [-1, 64, 105, 105]             128\n",
      "      BatchNorm2d-13         [-1, 64, 105, 105]             128\n",
      "             ReLU-14         [-1, 64, 105, 105]               0\n",
      "             ReLU-15         [-1, 64, 105, 105]               0\n",
      "           Conv2d-16         [-1, 64, 105, 105]          36,864\n",
      "           Conv2d-17         [-1, 64, 105, 105]          36,864\n",
      "      BatchNorm2d-18         [-1, 64, 105, 105]             128\n",
      "      BatchNorm2d-19         [-1, 64, 105, 105]             128\n",
      "             ReLU-20         [-1, 64, 105, 105]               0\n",
      "             ReLU-21         [-1, 64, 105, 105]               0\n",
      "           Conv2d-22        [-1, 256, 105, 105]          16,384\n",
      "           Conv2d-23        [-1, 256, 105, 105]          16,384\n",
      "      BatchNorm2d-24        [-1, 256, 105, 105]             512\n",
      "      BatchNorm2d-25        [-1, 256, 105, 105]             512\n",
      "           Conv2d-26        [-1, 256, 105, 105]          16,384\n",
      "           Conv2d-27        [-1, 256, 105, 105]          16,384\n",
      "      BatchNorm2d-28        [-1, 256, 105, 105]             512\n",
      "      BatchNorm2d-29        [-1, 256, 105, 105]             512\n",
      "             ReLU-30        [-1, 256, 105, 105]               0\n",
      "             ReLU-31        [-1, 256, 105, 105]               0\n",
      "       Bottleneck-32        [-1, 256, 105, 105]               0\n",
      "       Bottleneck-33        [-1, 256, 105, 105]               0\n",
      "           Conv2d-34         [-1, 64, 105, 105]          16,384\n",
      "           Conv2d-35         [-1, 64, 105, 105]          16,384\n",
      "      BatchNorm2d-36         [-1, 64, 105, 105]             128\n",
      "      BatchNorm2d-37         [-1, 64, 105, 105]             128\n",
      "             ReLU-38         [-1, 64, 105, 105]               0\n",
      "             ReLU-39         [-1, 64, 105, 105]               0\n",
      "           Conv2d-40         [-1, 64, 105, 105]          36,864\n",
      "           Conv2d-41         [-1, 64, 105, 105]          36,864\n",
      "      BatchNorm2d-42         [-1, 64, 105, 105]             128\n",
      "      BatchNorm2d-43         [-1, 64, 105, 105]             128\n",
      "             ReLU-44         [-1, 64, 105, 105]               0\n",
      "             ReLU-45         [-1, 64, 105, 105]               0\n",
      "           Conv2d-46        [-1, 256, 105, 105]          16,384\n",
      "           Conv2d-47        [-1, 256, 105, 105]          16,384\n",
      "      BatchNorm2d-48        [-1, 256, 105, 105]             512\n",
      "      BatchNorm2d-49        [-1, 256, 105, 105]             512\n",
      "             ReLU-50        [-1, 256, 105, 105]               0\n",
      "             ReLU-51        [-1, 256, 105, 105]               0\n",
      "       Bottleneck-52        [-1, 256, 105, 105]               0\n",
      "       Bottleneck-53        [-1, 256, 105, 105]               0\n",
      "           Conv2d-54         [-1, 64, 105, 105]          16,384\n",
      "           Conv2d-55         [-1, 64, 105, 105]          16,384\n",
      "      BatchNorm2d-56         [-1, 64, 105, 105]             128\n",
      "      BatchNorm2d-57         [-1, 64, 105, 105]             128\n",
      "             ReLU-58         [-1, 64, 105, 105]               0\n",
      "             ReLU-59         [-1, 64, 105, 105]               0\n",
      "           Conv2d-60         [-1, 64, 105, 105]          36,864\n",
      "           Conv2d-61         [-1, 64, 105, 105]          36,864\n",
      "      BatchNorm2d-62         [-1, 64, 105, 105]             128\n",
      "      BatchNorm2d-63         [-1, 64, 105, 105]             128\n",
      "             ReLU-64         [-1, 64, 105, 105]               0\n",
      "             ReLU-65         [-1, 64, 105, 105]               0\n",
      "           Conv2d-66        [-1, 256, 105, 105]          16,384\n",
      "           Conv2d-67        [-1, 256, 105, 105]          16,384\n",
      "      BatchNorm2d-68        [-1, 256, 105, 105]             512\n",
      "      BatchNorm2d-69        [-1, 256, 105, 105]             512\n",
      "             ReLU-70        [-1, 256, 105, 105]               0\n",
      "             ReLU-71        [-1, 256, 105, 105]               0\n",
      "       Bottleneck-72        [-1, 256, 105, 105]               0\n",
      "       Bottleneck-73        [-1, 256, 105, 105]               0\n",
      "           Conv2d-74        [-1, 256, 105, 105]          65,792\n",
      "             ReLU-75        [-1, 256, 105, 105]               0\n",
      "           Conv2d-76        [-1, 128, 105, 105]          32,768\n",
      "           Conv2d-77        [-1, 128, 105, 105]          32,768\n",
      "      BatchNorm2d-78        [-1, 128, 105, 105]             256\n",
      "      BatchNorm2d-79        [-1, 128, 105, 105]             256\n",
      "             ReLU-80        [-1, 128, 105, 105]               0\n",
      "             ReLU-81        [-1, 128, 105, 105]               0\n",
      "           Conv2d-82          [-1, 128, 53, 53]         147,456\n",
      "           Conv2d-83          [-1, 128, 53, 53]         147,456\n",
      "      BatchNorm2d-84          [-1, 128, 53, 53]             256\n",
      "      BatchNorm2d-85          [-1, 128, 53, 53]             256\n",
      "             ReLU-86          [-1, 128, 53, 53]               0\n",
      "             ReLU-87          [-1, 128, 53, 53]               0\n",
      "           Conv2d-88          [-1, 512, 53, 53]          65,536\n",
      "           Conv2d-89          [-1, 512, 53, 53]          65,536\n",
      "      BatchNorm2d-90          [-1, 512, 53, 53]           1,024\n",
      "      BatchNorm2d-91          [-1, 512, 53, 53]           1,024\n",
      "           Conv2d-92          [-1, 512, 53, 53]         131,072\n",
      "           Conv2d-93          [-1, 512, 53, 53]         131,072\n",
      "      BatchNorm2d-94          [-1, 512, 53, 53]           1,024\n",
      "      BatchNorm2d-95          [-1, 512, 53, 53]           1,024\n",
      "             ReLU-96          [-1, 512, 53, 53]               0\n",
      "             ReLU-97          [-1, 512, 53, 53]               0\n",
      "       Bottleneck-98          [-1, 512, 53, 53]               0\n",
      "       Bottleneck-99          [-1, 512, 53, 53]               0\n",
      "          Conv2d-100          [-1, 128, 53, 53]          65,536\n",
      "          Conv2d-101          [-1, 128, 53, 53]          65,536\n",
      "     BatchNorm2d-102          [-1, 128, 53, 53]             256\n",
      "     BatchNorm2d-103          [-1, 128, 53, 53]             256\n",
      "            ReLU-104          [-1, 128, 53, 53]               0\n",
      "            ReLU-105          [-1, 128, 53, 53]               0\n",
      "          Conv2d-106          [-1, 128, 53, 53]         147,456\n",
      "          Conv2d-107          [-1, 128, 53, 53]         147,456\n",
      "     BatchNorm2d-108          [-1, 128, 53, 53]             256\n",
      "     BatchNorm2d-109          [-1, 128, 53, 53]             256\n",
      "            ReLU-110          [-1, 128, 53, 53]               0\n",
      "            ReLU-111          [-1, 128, 53, 53]               0\n",
      "          Conv2d-112          [-1, 512, 53, 53]          65,536\n",
      "          Conv2d-113          [-1, 512, 53, 53]          65,536\n",
      "     BatchNorm2d-114          [-1, 512, 53, 53]           1,024\n",
      "     BatchNorm2d-115          [-1, 512, 53, 53]           1,024\n",
      "            ReLU-116          [-1, 512, 53, 53]               0\n",
      "            ReLU-117          [-1, 512, 53, 53]               0\n",
      "      Bottleneck-118          [-1, 512, 53, 53]               0\n",
      "      Bottleneck-119          [-1, 512, 53, 53]               0\n",
      "          Conv2d-120          [-1, 128, 53, 53]          65,536\n",
      "          Conv2d-121          [-1, 128, 53, 53]          65,536\n",
      "     BatchNorm2d-122          [-1, 128, 53, 53]             256\n",
      "     BatchNorm2d-123          [-1, 128, 53, 53]             256\n",
      "            ReLU-124          [-1, 128, 53, 53]               0\n",
      "            ReLU-125          [-1, 128, 53, 53]               0\n",
      "          Conv2d-126          [-1, 128, 53, 53]         147,456\n",
      "          Conv2d-127          [-1, 128, 53, 53]         147,456\n",
      "     BatchNorm2d-128          [-1, 128, 53, 53]             256\n",
      "     BatchNorm2d-129          [-1, 128, 53, 53]             256\n",
      "            ReLU-130          [-1, 128, 53, 53]               0\n",
      "            ReLU-131          [-1, 128, 53, 53]               0\n",
      "          Conv2d-132          [-1, 512, 53, 53]          65,536\n",
      "          Conv2d-133          [-1, 512, 53, 53]          65,536\n",
      "     BatchNorm2d-134          [-1, 512, 53, 53]           1,024\n",
      "     BatchNorm2d-135          [-1, 512, 53, 53]           1,024\n",
      "            ReLU-136          [-1, 512, 53, 53]               0\n",
      "            ReLU-137          [-1, 512, 53, 53]               0\n",
      "      Bottleneck-138          [-1, 512, 53, 53]               0\n",
      "      Bottleneck-139          [-1, 512, 53, 53]               0\n",
      "          Conv2d-140          [-1, 128, 53, 53]          65,536\n",
      "          Conv2d-141          [-1, 128, 53, 53]          65,536\n",
      "     BatchNorm2d-142          [-1, 128, 53, 53]             256\n",
      "     BatchNorm2d-143          [-1, 128, 53, 53]             256\n",
      "            ReLU-144          [-1, 128, 53, 53]               0\n",
      "            ReLU-145          [-1, 128, 53, 53]               0\n",
      "          Conv2d-146          [-1, 128, 53, 53]         147,456\n",
      "          Conv2d-147          [-1, 128, 53, 53]         147,456\n",
      "     BatchNorm2d-148          [-1, 128, 53, 53]             256\n",
      "     BatchNorm2d-149          [-1, 128, 53, 53]             256\n",
      "            ReLU-150          [-1, 128, 53, 53]               0\n",
      "            ReLU-151          [-1, 128, 53, 53]               0\n",
      "          Conv2d-152          [-1, 512, 53, 53]          65,536\n",
      "          Conv2d-153          [-1, 512, 53, 53]          65,536\n",
      "     BatchNorm2d-154          [-1, 512, 53, 53]           1,024\n",
      "     BatchNorm2d-155          [-1, 512, 53, 53]           1,024\n",
      "            ReLU-156          [-1, 512, 53, 53]               0\n",
      "            ReLU-157          [-1, 512, 53, 53]               0\n",
      "      Bottleneck-158          [-1, 512, 53, 53]               0\n",
      "      Bottleneck-159          [-1, 512, 53, 53]               0\n",
      "          Conv2d-160          [-1, 512, 53, 53]         262,656\n",
      "            ReLU-161          [-1, 512, 53, 53]               0\n",
      "          Conv2d-162          [-1, 256, 53, 53]         131,072\n",
      "          Conv2d-163          [-1, 256, 53, 53]         131,072\n",
      "     BatchNorm2d-164          [-1, 256, 53, 53]             512\n",
      "     BatchNorm2d-165          [-1, 256, 53, 53]             512\n",
      "            ReLU-166          [-1, 256, 53, 53]               0\n",
      "            ReLU-167          [-1, 256, 53, 53]               0\n",
      "          Conv2d-168          [-1, 256, 27, 27]         589,824\n",
      "          Conv2d-169          [-1, 256, 27, 27]         589,824\n",
      "     BatchNorm2d-170          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-171          [-1, 256, 27, 27]             512\n",
      "            ReLU-172          [-1, 256, 27, 27]               0\n",
      "            ReLU-173          [-1, 256, 27, 27]               0\n",
      "          Conv2d-174         [-1, 1024, 27, 27]         262,144\n",
      "          Conv2d-175         [-1, 1024, 27, 27]         262,144\n",
      "     BatchNorm2d-176         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-177         [-1, 1024, 27, 27]           2,048\n",
      "          Conv2d-178         [-1, 1024, 27, 27]         524,288\n",
      "          Conv2d-179         [-1, 1024, 27, 27]         524,288\n",
      "     BatchNorm2d-180         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-181         [-1, 1024, 27, 27]           2,048\n",
      "            ReLU-182         [-1, 1024, 27, 27]               0\n",
      "            ReLU-183         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-184         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-185         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-186          [-1, 256, 27, 27]         262,144\n",
      "          Conv2d-187          [-1, 256, 27, 27]         262,144\n",
      "     BatchNorm2d-188          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-189          [-1, 256, 27, 27]             512\n",
      "            ReLU-190          [-1, 256, 27, 27]               0\n",
      "            ReLU-191          [-1, 256, 27, 27]               0\n",
      "          Conv2d-192          [-1, 256, 27, 27]         589,824\n",
      "          Conv2d-193          [-1, 256, 27, 27]         589,824\n",
      "     BatchNorm2d-194          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-195          [-1, 256, 27, 27]             512\n",
      "            ReLU-196          [-1, 256, 27, 27]               0\n",
      "            ReLU-197          [-1, 256, 27, 27]               0\n",
      "          Conv2d-198         [-1, 1024, 27, 27]         262,144\n",
      "          Conv2d-199         [-1, 1024, 27, 27]         262,144\n",
      "     BatchNorm2d-200         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-201         [-1, 1024, 27, 27]           2,048\n",
      "            ReLU-202         [-1, 1024, 27, 27]               0\n",
      "            ReLU-203         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-204         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-205         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-206          [-1, 256, 27, 27]         262,144\n",
      "          Conv2d-207          [-1, 256, 27, 27]         262,144\n",
      "     BatchNorm2d-208          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-209          [-1, 256, 27, 27]             512\n",
      "            ReLU-210          [-1, 256, 27, 27]               0\n",
      "            ReLU-211          [-1, 256, 27, 27]               0\n",
      "          Conv2d-212          [-1, 256, 27, 27]         589,824\n",
      "          Conv2d-213          [-1, 256, 27, 27]         589,824\n",
      "     BatchNorm2d-214          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-215          [-1, 256, 27, 27]             512\n",
      "            ReLU-216          [-1, 256, 27, 27]               0\n",
      "            ReLU-217          [-1, 256, 27, 27]               0\n",
      "          Conv2d-218         [-1, 1024, 27, 27]         262,144\n",
      "          Conv2d-219         [-1, 1024, 27, 27]         262,144\n",
      "     BatchNorm2d-220         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-221         [-1, 1024, 27, 27]           2,048\n",
      "            ReLU-222         [-1, 1024, 27, 27]               0\n",
      "            ReLU-223         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-224         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-225         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-226          [-1, 256, 27, 27]         262,144\n",
      "          Conv2d-227          [-1, 256, 27, 27]         262,144\n",
      "     BatchNorm2d-228          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-229          [-1, 256, 27, 27]             512\n",
      "            ReLU-230          [-1, 256, 27, 27]               0\n",
      "            ReLU-231          [-1, 256, 27, 27]               0\n",
      "          Conv2d-232          [-1, 256, 27, 27]         589,824\n",
      "          Conv2d-233          [-1, 256, 27, 27]         589,824\n",
      "     BatchNorm2d-234          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-235          [-1, 256, 27, 27]             512\n",
      "            ReLU-236          [-1, 256, 27, 27]               0\n",
      "            ReLU-237          [-1, 256, 27, 27]               0\n",
      "          Conv2d-238         [-1, 1024, 27, 27]         262,144\n",
      "          Conv2d-239         [-1, 1024, 27, 27]         262,144\n",
      "     BatchNorm2d-240         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-241         [-1, 1024, 27, 27]           2,048\n",
      "            ReLU-242         [-1, 1024, 27, 27]               0\n",
      "            ReLU-243         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-244         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-245         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-246          [-1, 256, 27, 27]         262,144\n",
      "          Conv2d-247          [-1, 256, 27, 27]         262,144\n",
      "     BatchNorm2d-248          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-249          [-1, 256, 27, 27]             512\n",
      "            ReLU-250          [-1, 256, 27, 27]               0\n",
      "            ReLU-251          [-1, 256, 27, 27]               0\n",
      "          Conv2d-252          [-1, 256, 27, 27]         589,824\n",
      "          Conv2d-253          [-1, 256, 27, 27]         589,824\n",
      "     BatchNorm2d-254          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-255          [-1, 256, 27, 27]             512\n",
      "            ReLU-256          [-1, 256, 27, 27]               0\n",
      "            ReLU-257          [-1, 256, 27, 27]               0\n",
      "          Conv2d-258         [-1, 1024, 27, 27]         262,144\n",
      "          Conv2d-259         [-1, 1024, 27, 27]         262,144\n",
      "     BatchNorm2d-260         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-261         [-1, 1024, 27, 27]           2,048\n",
      "            ReLU-262         [-1, 1024, 27, 27]               0\n",
      "            ReLU-263         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-264         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-265         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-266          [-1, 256, 27, 27]         262,144\n",
      "          Conv2d-267          [-1, 256, 27, 27]         262,144\n",
      "     BatchNorm2d-268          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-269          [-1, 256, 27, 27]             512\n",
      "            ReLU-270          [-1, 256, 27, 27]               0\n",
      "            ReLU-271          [-1, 256, 27, 27]               0\n",
      "          Conv2d-272          [-1, 256, 27, 27]         589,824\n",
      "          Conv2d-273          [-1, 256, 27, 27]         589,824\n",
      "     BatchNorm2d-274          [-1, 256, 27, 27]             512\n",
      "     BatchNorm2d-275          [-1, 256, 27, 27]             512\n",
      "            ReLU-276          [-1, 256, 27, 27]               0\n",
      "            ReLU-277          [-1, 256, 27, 27]               0\n",
      "          Conv2d-278         [-1, 1024, 27, 27]         262,144\n",
      "          Conv2d-279         [-1, 1024, 27, 27]         262,144\n",
      "     BatchNorm2d-280         [-1, 1024, 27, 27]           2,048\n",
      "     BatchNorm2d-281         [-1, 1024, 27, 27]           2,048\n",
      "            ReLU-282         [-1, 1024, 27, 27]               0\n",
      "            ReLU-283         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-284         [-1, 1024, 27, 27]               0\n",
      "      Bottleneck-285         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-286         [-1, 1024, 27, 27]       1,049,600\n",
      "            ReLU-287         [-1, 1024, 27, 27]               0\n",
      "          Conv2d-288          [-1, 512, 27, 27]         524,288\n",
      "          Conv2d-289          [-1, 512, 27, 27]         524,288\n",
      "     BatchNorm2d-290          [-1, 512, 27, 27]           1,024\n",
      "     BatchNorm2d-291          [-1, 512, 27, 27]           1,024\n",
      "            ReLU-292          [-1, 512, 27, 27]               0\n",
      "            ReLU-293          [-1, 512, 27, 27]               0\n",
      "          Conv2d-294          [-1, 512, 14, 14]       2,359,296\n",
      "          Conv2d-295          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-296          [-1, 512, 14, 14]           1,024\n",
      "     BatchNorm2d-297          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-298          [-1, 512, 14, 14]               0\n",
      "            ReLU-299          [-1, 512, 14, 14]               0\n",
      "          Conv2d-300         [-1, 2048, 14, 14]       1,048,576\n",
      "          Conv2d-301         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-302         [-1, 2048, 14, 14]           4,096\n",
      "     BatchNorm2d-303         [-1, 2048, 14, 14]           4,096\n",
      "          Conv2d-304         [-1, 2048, 14, 14]       2,097,152\n",
      "          Conv2d-305         [-1, 2048, 14, 14]       2,097,152\n",
      "     BatchNorm2d-306         [-1, 2048, 14, 14]           4,096\n",
      "     BatchNorm2d-307         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-308         [-1, 2048, 14, 14]               0\n",
      "            ReLU-309         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-311         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-312          [-1, 512, 14, 14]       1,048,576\n",
      "          Conv2d-313          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-314          [-1, 512, 14, 14]           1,024\n",
      "     BatchNorm2d-315          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-316          [-1, 512, 14, 14]               0\n",
      "            ReLU-317          [-1, 512, 14, 14]               0\n",
      "          Conv2d-318          [-1, 512, 14, 14]       2,359,296\n",
      "          Conv2d-319          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-320          [-1, 512, 14, 14]           1,024\n",
      "     BatchNorm2d-321          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-322          [-1, 512, 14, 14]               0\n",
      "            ReLU-323          [-1, 512, 14, 14]               0\n",
      "          Conv2d-324         [-1, 2048, 14, 14]       1,048,576\n",
      "          Conv2d-325         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-326         [-1, 2048, 14, 14]           4,096\n",
      "     BatchNorm2d-327         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-328         [-1, 2048, 14, 14]               0\n",
      "            ReLU-329         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-330         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-331         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-332          [-1, 512, 14, 14]       1,048,576\n",
      "          Conv2d-333          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-334          [-1, 512, 14, 14]           1,024\n",
      "     BatchNorm2d-335          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-336          [-1, 512, 14, 14]               0\n",
      "            ReLU-337          [-1, 512, 14, 14]               0\n",
      "          Conv2d-338          [-1, 512, 14, 14]       2,359,296\n",
      "          Conv2d-339          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-340          [-1, 512, 14, 14]           1,024\n",
      "     BatchNorm2d-341          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-342          [-1, 512, 14, 14]               0\n",
      "            ReLU-343          [-1, 512, 14, 14]               0\n",
      "          Conv2d-344         [-1, 2048, 14, 14]       1,048,576\n",
      "          Conv2d-345         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-346         [-1, 2048, 14, 14]           4,096\n",
      "     BatchNorm2d-347         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-348         [-1, 2048, 14, 14]               0\n",
      "            ReLU-349         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-350         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-351         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-352         [-1, 2048, 14, 14]       4,196,352\n",
      "            ReLU-353         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-354           [-1, 4096, 8, 8]     411,045,888\n",
      "            ReLU-355           [-1, 4096, 8, 8]               0\n",
      "          Conv2d-356           [-1, 4096, 8, 8]      16,781,312\n",
      "            ReLU-357           [-1, 4096, 8, 8]               0\n",
      "          Conv2d-358             [-1, 20, 8, 8]          81,920\n",
      " ConvTranspose2d-359           [-1, 20, 16, 16]           6,400\n",
      "          Conv2d-360           [-1, 20, 27, 27]          20,480\n",
      " ConvTranspose2d-361           [-1, 20, 32, 32]           6,400\n",
      "          Conv2d-362           [-1, 20, 53, 53]          10,240\n",
      " ConvTranspose2d-363         [-1, 20, 256, 256]         102,400\n",
      "================================================================\n",
      "Total params: 480,640,256\n",
      "Trainable params: 480,640,256\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2149.86\n",
      "Params size (MB): 1833.50\n",
      "Estimated Total Size (MB): 3983.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "net = MyFCN(20)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "summary(net, input_size=(3, 224, 224))\n",
    "#x = Variable(torch.randn(1,3,300,300))\n",
    "\n",
    "#model(x)\n",
    "#vis_graph = make_dot(model(x), params=dict(model.named_parameters()))\n",
    "#vis_graph.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 300, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.randn(1,3,300,1)).cuda()\n",
    "result = model(x)\n",
    "result.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割线\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_root = '/home/hyfok/Notebook/LIP'\n",
    "\n",
    "def read_images(root=voc_root, train=True):\n",
    "    txt_fname = root + '/TrainVal_images/' + ('train_id.txt' if train else 'val_id.txt')\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    data = [os.path.join(root, 'TrainVal_images/TrainVal_images', ('train_images/' if train else 'val_images/')+i+'.jpg') for i in images]\n",
    "    label = [os.path.join(root, 'TrainVal_parsing_annotations/TrainVal_parsing_annotations', ('train_segmentations/' if train else 'val_segmentations/')+i+'.png') for i in images]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_crop(data, label, height, width):\n",
    "    \n",
    "    #data is PIL.Image object\n",
    "    #label is PIL.Image object\n",
    "    \n",
    "    data, rect = transforms.RandomCrop((height, width))(data)\n",
    "    label = transforms.FixedCrop(*rect)(label)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['background','Hat','Hair','Glove','Sunglasses','UpperClothes',\n",
    "           'Dress','Coat','Socks','Pants','Jumpsuits','Scarf','Skirt',\n",
    "           'Face','Left-arm','Right-arm','Left-leg','Right-leg',\n",
    "           'Left-shoe','Right-shoe']\n",
    "\n",
    "# RGB color for each class\n",
    "colormap = [[0,0,0],[128,0,0],[0,128,0], [128,128,0], [0,0,128],\n",
    "            [128,0,128],[0,128,128],[128,128,128],[64,0,0],[192,0,0],\n",
    "            [64,128,0],[192,128,0],[64,0,128],[192,0,128],\n",
    "            [64,128,128],[192,128,128],[0,64,0],[128,64,0],\n",
    "            [0,192,0],[128,192,128]]\n",
    "\n",
    "len(classes), len(colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png2color(im):\n",
    "    data = np.array(im, dtype='int32')\n",
    "    if((data[:,:,0] == data[:,:,1]).all() and (data[:,:,0] == data[:,:,2]).all()):\n",
    "        idx = data[:, :, 0]\n",
    "        return np.array([colormap[i] for cols in idx.tolist() for i in cols], dtype='uint8').reshape(idx.shape[0],idx.shape[1],3) # 根据索引得到color图\n",
    "    return []\n",
    "def png2label(im):\n",
    "    data = np.array(im, dtype='int32')   \n",
    "    label = np.zeros((data.shape[0],data.shape[1],20)).tolist()\n",
    "    for i in range(len(label)):\n",
    "        for j in range(len(label[0])):\n",
    "            label[i][j][data[i][j][0]] = 1\n",
    "    return np.transpose(np.array(label,dtype='float32'),(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 120, 104)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_im = Image.open('/home/hyfok/Notebook/LIP/TrainVal_parsing_annotations/TrainVal_parsing_annotations/train_segmentations/77_471474.png').convert('RGB')\n",
    "png2label(label_im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAAB4CAIAAACRqCBtAAADmUlEQVR4nO2cSXLcMAxFmZQPhqPhaDxaFqzIUoviAIIY1HiVhdJWS+inDw6x45SCQJI/1VcB8TjOp+Pg4Oc4ghA0w9/uGSG0ym+rtgVFw34Y6Ceu8M25y4j33IyKCz6ozKrRsx9UP/J04gDxm9v2IFqVyM/9payeKbwdcHDuuPUPWN85jFyaebCrXozpDvdS18U9tmrXC+dgx3QZSVpjnP4Eiv//2KMyxkmDY+eMnHZj37PvzKojDctVCjuN4teFPk4OhREvgCmlhLORmDx9/C3jUlaeOts6DhNOu+Omuqlsn0++1+oYB/RbcyI/jy0l7m7NQu5k2DKrCutrxA0emnd9TuuIa6y5203K7+56sfxbxtxduJYB04krFY9IOZ+zLrFcAdPc8J+2LZiEFsCLEhExpQSYspkBlDg5MLQhju4HUHLXPIyBLdcDI74WT1hBWtwR1fvB5bTKpDRxF4EA8oibnUMvQx7ThxTuVrnEtSPmjs4mv4Hw57+3akbNZ2B3cvggo3YFV+h71Xd0HBl6q57Rkqj48Nj+PU4e3cjziPvCtvWaOPVH5TJx6taSo+VIwYKygtdWVWd1OSITATtBO3CQOIPWkvExzqaygt3EWbaWLIszTogjQhzjjPeRAJE4IiGOyNICeF/D2h8KLCbOvrVkcAEMCXP9dVtQxG1KBHgI2oHFVq2StQv4YFpcxK1gInGD1vLeKuYwIc4j+uLcNWlBX5xTlMU5jVuaFae7pgfFe9+IViWis+UidCjwV7GEQuL8jmtnJsQpDnCgdeNnHIxxoF1AFWlx7+jT5CJxNhEV94LJ9MB04kC7gAZy4l4zuhVMJ84yQuJeFrckI45mDZirYGbvXvV9QTuY+E5+vhxj40wWX7B+iZ0QxQkAsrebZXSMyzuL8MiQuLy5CI/EOo5IX1zeX4RHInFElH/MK9f+xzhO/aoMJeTEVR35ZaM4mB8fXWSt0Ps1aJOXg9pf8V1ZK/AkDjgu4ihuSX1ySN58HXT2qnnsKtA7odqtTpUVGBIH829xrazAkDhgKMMfnZ0DiBThkdhyEQlxREIckY64LFKERyJxREIckRBHJMQRCXFEYudAJBJHJNZxRCJxRFrislQRHllNHHAU4ZFoVSIhjkiII/LwPYd8OoTW+5tffDO1xGXpIjzSb1XIz1/iq8MdtVbNw+8GrjL8EZMDkZo4kC7CI5E4IgvigK0Ij5B+6AaYi/BItCqRf45c0dbkGPReAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=104x120 at 0x7FA95DA66A58>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_im = Image.open('/home/hyfok/Notebook/LIP/TrainVal_parsing_annotations/TrainVal_parsing_annotations/train_segmentations/77_471474.png').convert('RGB')\n",
    "color_image = png2color(label_im)\n",
    "Image.fromarray(color_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transforms(im, label):\n",
    "    #im, label = rand_crop(im, label, *crop_size)\n",
    "    im_tfs = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    im = im_tfs(im)\n",
    "    label = png2label(label)\n",
    "    label = torch.from_numpy(label)\n",
    "    return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIPSegDataset(Dataset):\n",
    "    \n",
    "    # LIP dataset\n",
    "    \n",
    "    def __init__(self, train, transforms):\n",
    "        #self.crop_size = crop_size\n",
    "        self.transforms = transforms\n",
    "        data_list, label_list = read_images(train=train)\n",
    "        self.data_list = data_list\n",
    "        self.label_list = label_list\n",
    "        print('Read ' + str(len(self.data_list)) + ' images')\n",
    "        \n",
    "    def _filter(self, images): # 过滤掉图片大小小于 crop 大小的图片\n",
    "        return [im for im in images if (Image.open(im).size[1] >= self.crop_size[0] and \n",
    "                                        Image.open(im).size[0] >= self.crop_size[1])]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        img = Image.open(img)\n",
    "        label = Image.open(label).convert('RGB')\n",
    "        img, label = self.transforms(img, label)\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 30462 images\n",
      "Read 10000 images\n"
     ]
    }
   ],
   "source": [
    "# 实例化数据集\n",
    "\n",
    "lip_train = LIPSegDataset(True, img_transforms)\n",
    "lip_test = LIPSegDataset(False, img_transforms)\n",
    "\n",
    "train_data = DataLoader(lip_train, 1, shuffle=True, num_workers=8)\n",
    "valid_data = DataLoader(lip_test, 1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CrossMapLRN2d in module torch.nn.modules.normalization:\n",
      "\n",
      "class CrossMapLRN2d(torch.nn.modules.module.Module)\n",
      " |  CrossMapLRN2d(size, alpha=0.0001, beta=0.75, k=1)\n",
      " |  \n",
      " |  Base class for all neural network modules.\n",
      " |  \n",
      " |  Your models should also subclass this class.\n",
      " |  \n",
      " |  Modules can also contain other Modules, allowing to nest them in\n",
      " |  a tree structure. You can assign the submodules as regular attributes::\n",
      " |  \n",
      " |      import torch.nn as nn\n",
      " |      import torch.nn.functional as F\n",
      " |  \n",
      " |      class Model(nn.Module):\n",
      " |          def __init__(self):\n",
      " |              super(Model, self).__init__()\n",
      " |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      " |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      " |  \n",
      " |          def forward(self, x):\n",
      " |              x = F.relu(self.conv1(x))\n",
      " |              return F.relu(self.conv2(x))\n",
      " |  \n",
      " |  Submodules assigned in this way will be registered, and will have their\n",
      " |  parameters converted too when you call :meth:`to`, etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CrossMapLRN2d\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, size, alpha=0.0001, beta=0.75, k=1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf.data), buf.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.CrossMapLRN2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def label_accuracy_score(label_trues, label_preds, n_class):\n",
    "    # Returns accuracy score evaluation result.\n",
    "      # overall accuracy\n",
    "      # mean accuracy\n",
    "      # mean IU\n",
    "      # fwavacc\n",
    "\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割线\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "gamma      = 0.5\n",
    "epochs     = 500\n",
    "lr         = 1e-4\n",
    "momentum   = 0\n",
    "w_decay    = 1e-5\n",
    "step_size  = 50\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "basic_optim = optim.RMSprop(net.parameters(), lr=lr, momentum=momentum, weight_decay=w_decay)\n",
    "#optimizer = lr_scheduler.StepLR(basic_optim, step_size=step_size, gamma=gamma)  # decay LR by a factor of 0.5 every 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.dtype,label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StepLR' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b289b4460131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StepLR' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "for e in range(80):\n",
    "    if e > 0 and e % 50 == 0:\n",
    "        optimizer.set_learning_rate(optimizer.learning_rate * 0.1)\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_acc_cls = 0\n",
    "    train_mean_iu = 0\n",
    "    train_fwavacc = 0\n",
    "    \n",
    "    prev_time = datetime.now()\n",
    "    net = net.train()\n",
    "    for data in train_data:\n",
    "        im = Variable(data[0].cuda())\n",
    "        label = Variable(data[1].cuda())\n",
    "        # forward\n",
    "        out = net(im)\n",
    "        out = F.log_softmax(out, dim=1) # (b, n, h, w)\n",
    "        loss = criterion(out, label)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        label_pred = out.max(dim=1)[1].data.cpu().numpy()\n",
    "        label_true = label.data.cpu().numpy()\n",
    "        for lbt, lbp in zip(label_true, label_pred):\n",
    "            acc, acc_cls, mean_iu, fwavacc = label_accuracy_score(lbt, lbp, num_classes)\n",
    "            train_acc += acc\n",
    "            train_acc_cls += acc_cls\n",
    "            train_mean_iu += mean_iu\n",
    "            train_fwavacc += fwavacc\n",
    "        \n",
    "    net = net.eval()\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    eval_acc_cls = 0\n",
    "    eval_mean_iu = 0\n",
    "    eval_fwavacc = 0\n",
    "    for data in valid_data:\n",
    "        im = Variable(data[0].cuda(), volatile=True)\n",
    "        label = Variable(data[1].cuda(), volatile=True)\n",
    "        # forward\n",
    "        out = net(im)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.data[0]\n",
    "        \n",
    "        label_pred = out.max(dim=1)[1].data.cpu().numpy()\n",
    "        label_true = label.data.cpu().numpy()\n",
    "        for lbt, lbp in zip(label_true, label_pred):\n",
    "            acc, acc_cls, mean_iu, fwavacc = label_accuracy_score(lbt, lbp, num_classes)\n",
    "            eval_acc += acc\n",
    "            eval_acc_cls += acc_cls\n",
    "            eval_mean_iu += mean_iu\n",
    "            eval_fwavacc += fwavacc\n",
    "        \n",
    "    cur_time = datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    epoch_str = ('Epoch: {}, Train Loss: {:.5f}, Train Acc: {:.5f}, Train Mean IU: {:.5f}, \\\n",
    "Valid Loss: {:.5f}, Valid Acc: {:.5f}, Valid Mean IU: {:.5f} '.format(\n",
    "        e, train_loss / len(train_data), train_acc / len(voc_train), train_mean_iu / len(voc_train),\n",
    "        eval_loss / len(valid_data), eval_acc / len(voc_test), eval_mean_iu / len(voc_test)))\n",
    "    time_str = 'Time: {:.0f}:{:.0f}:{:.0f}'.format(h, m, s)\n",
    "    print(epoch_str + time_str + ' lr: {}'.format(optimizer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
