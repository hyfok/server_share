{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_root = '/home/hyfok/Notebook/LIP'\n",
    "\n",
    "def read_images(root=voc_root, train=True):\n",
    "    txt_fname = root + '/TrainVal_images/' + ('train_id.txt' if train else 'val_id.txt')\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    data = [os.path.join(root, 'TrainVal_images/TrainVal_images', ('train_images' if train else 'val_images')+i+'.jpg') for i in images]\n",
    "    label = [os.path.join(root, 'TrainVal_parsing_annotations/TrainVal_parsing_annotations', ('train_segmentations' if train else 'val_segmentations')+i+'.png') for i in images]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_crop(data, label, height, width):\n",
    "    '''\n",
    "    data is PIL.Image object\n",
    "    label is PIL.Image object\n",
    "    '''\n",
    "    data, rect = tfs.RandomCrop((height, width))(data)\n",
    "    label = tfs.FixedCrop(*rect)(label)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['background','Hat','Hair','Glove','Sunglasses','UpperClothes',\n",
    "           'Dress','Coat','Socks','Pants','Jumpsuits','Scarf','Skirt',\n",
    "           'Face','Left-arm','Right-arm','Left-leg','Right-leg',\n",
    "           'Left-shoe','Right-shoe']\n",
    "\n",
    "# RGB color for each class\n",
    "colormap = [[0,0,0],[128,0,0],[0,128,0], [128,128,0], [0,0,128],\n",
    "            [128,0,128],[0,128,128],[128,128,128],[64,0,0],[192,0,0],\n",
    "            [64,128,0],[192,128,0],[64,0,128],[192,0,128],\n",
    "            [64,128,128],[192,128,128],[0,64,0],[128,64,0],\n",
    "            [0,192,0],[128,192,0]]\n",
    "\n",
    "len(classes), len(colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2color(im):\n",
    "    data = np.array(im, dtype='int32')\n",
    "    if((data[:,:,0] == data[:,:,1]).all() and (data[:,:,0] == data[:,:,2]).all()):\n",
    "        idx = data[:, :, 0]\n",
    "        return np.array([colormap[i] for cols in idx.tolist() for i in cols], dtype='int64').reshape(idx.shape[0],idx.shape[1],3) # 根据索引得到color图\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAAB4CAIAAACRqCBtAAADmElEQVR4nO2cSZLbMAxFmVQfDEfD0Xi0LFhRSxbFAQQxyHiVheKWJfjpg0PalZSCQJI/1VcB8TjOp+Pg4Oc4ghA0w9/uGSG0ym+rtgVFw34Y6Ceu8M25y4j33IyKCz6ozKrRsx9UP/J04gDxm9v2IFqVyM/9payeKbwdcHDuuPUPWN85jFyaebCrXozpDvdS18U9tmrXC+dgx3QZSVpjnP4Eiv//2KMyxkmDY+eMnHZj37PvzKojDctVCjuN4teFPk4OhREvgCmlhLORmDx9/C3jUlaeOts6DhNOu+Omuqlsn0++1+oYB/RbcyI/jy0l7m7NQu5k2DKrCutrxA0emnd9TuuIa6y5203K7+56sfxbxtxduJYB04krFY9IOZ+zLrFcAdPc8J+2LZiEFsCLEhExpQSYspkBlDg5MLQhju4HUHLXPIyBLdcDI74WT1hBWtwR1fvB5bTKpDRxF4EA8oibnUMvQx7ThxTuVrnEtSPmjs4mv4Hw57+3akbNZ2B3cvggo3YFV+h71Xd0HBl6q57Rkqj48Nj+PU4e3cjziPvCtvWaOPVH5TJx6taSo+VIwYKygtdWVWd1OSITATtBO3CQOIPWkvExzqaygt3EWbaWLIszTogjQhzjjPeRAJE4IiGOyNICeF/D2h8KLCbOvrVkcAEMCXP9dVtQxG1KBHgI2oHFVq2StQv4YFpcxK1gInGD1vLeKuYwIc4j+uLcNWlBX5xTlMU5jVuaFae7pgfFe9+IViWis+UidCjwV7GEQuL8jmtnJsQpDnCgdeNnHIxxoF1AFWlx7+jT5CJxNhEV94LJ9MB04kC7gAZy4l4zuhVMJ84yQuJeFrckI45mDZirYGbvXvV9QTuY+E1+vhxj40wWX7B+iZ0QxQkAsrebZXSMyzuL8MiQuLy5CI/EOo5IX1zeX4RHInFElL/mlXPlRQThKijIias68stGcTA/PrrIWqH336BNXg5qf61/xdI5PIkDjos4iltSnxySN18Hnb1qHrsK9E6odqtTZQWGxMH8W1wrKzAkDhjK8Edn5wAiRXgktlxEQhyREEekIy6LFOGRSByREEckxBEJcURCHJHYORCJxBGJdRyRSByRlrgsVYRHVhMHHEV4JFqVSIgjEuKIPPzOIZ8OofX+5g/fTC1xWboIj/RbFfLzj/jqcEetVfPwu4GrDH/E5ECkJg6ki/BIJI7IgjhgK8IjpC/dAHMRHolWJfIPcZ3M1jD9vLMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=104x120 at 0x7FEE125F9400>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_im = Image.open('/home/hyfok/Notebook/LIP/TrainVal_parsing_annotations/TrainVal_parsing_annotations/train_segmentations/77_471474.png').convert('RGB')\n",
    "color_image = label2color(label_im)\n",
    "Image.fromarray(color_image.astype('uint8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
